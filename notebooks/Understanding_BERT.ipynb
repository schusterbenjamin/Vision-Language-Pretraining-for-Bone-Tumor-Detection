{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd36300b",
   "metadata": {},
   "source": [
    "# Understanding BERT and how to use it :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c500d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer, BertModel, BertConfig, BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e784676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# caption = \"An x-ray image of a shoulder with an abnoramility\"\n",
    "caption = \"Pizza is amazing\"\n",
    "caption2 = \"Pizza is amazing\"\n",
    "caption3 = \"Everthing is going to end. But I am sure that this is not the end yet\"\n",
    "\n",
    "captions = [caption, caption2, caption3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76f31b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokens = tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=True, max_length=20)\n",
    "print(tokens)\n",
    "print(tokens.input_ids.shape)\n",
    "\n",
    "configuration = DistilBertConfig()\n",
    "distilbert = DistilBertModel(configuration)\n",
    "distilbert.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = distilbert(**tokens)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "print(last_hidden_states.shape)\n",
    "\n",
    "sentence_embeddings = last_hidden_states[:, 0, :]  # Use the first token (CLS token) for sentence embeddings\n",
    "\n",
    "print(sentence_embeddings.shape)\n",
    "cosine_similarities = sentence_embeddings @ sentence_embeddings.T\n",
    "cosine_similarities = cosine_similarities / (sentence_embeddings.norm(dim=1, keepdim=True) * sentence_embeddings.norm(dim=1, keepdim=True).T)\n",
    "print(cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88595ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokens = tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=True, max_length=20)\n",
    "print('Tokens:', tokens)\n",
    "print('Tokens input ids shape:', tokens.input_ids.shape)\n",
    "\n",
    "configuration = BertConfig()\n",
    "bert = BertModel(configuration)\n",
    "bert.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = bert(**tokens, output_attentions=True)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "print('Last hidden states shape:', last_hidden_states.shape)\n",
    "\n",
    "print('Attention shape:', outputs.attentions[0].shape)  # Shape of the first attention layer\n",
    "print('Attentions:', outputs.attentions)\n",
    "\n",
    "sentence_embeddings = last_hidden_states[:, 0, :]  # Use the first token (CLS token) for sentence embeddings\n",
    "\n",
    "print(sentence_embeddings.shape)\n",
    "cosine_similarities = sentence_embeddings @ sentence_embeddings.T\n",
    "cosine_similarities = cosine_similarities / (sentence_embeddings.norm(dim=1, keepdim=True) * sentence_embeddings.norm(dim=1, keepdim=True).T)\n",
    "print(cosine_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb9aa36",
   "metadata": {},
   "source": [
    "# Trying a simple training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e87934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class CLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenizer):\n",
    "        \"\"\"\n",
    "        image_filenames and cpations must have the same length; so, if there are\n",
    "        multiple captions for each image, the image_filenames must have repetitive\n",
    "        file names \n",
    "        \"\"\"\n",
    "\n",
    "        dataset = load_dataset(\"jxie/flickr8k\", split=\"train[:256]\")\n",
    "\n",
    "        self.images = dataset[:][\"image\"]\n",
    "        self.captions = dataset[:][\"caption_0\"]\n",
    "\n",
    "        self.encoded_captions = tokenizer(\n",
    "            list(self.captions), padding=True, truncation=True, max_length=1000 # TODO\n",
    "        )\n",
    "\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),      # Resize to fixed size\n",
    "            transforms.ToTensor(),              # Convert to tensor, scales to [0,1]\n",
    "            transforms.Normalize(               # Normalize with ImageNet stats\n",
    "                mean=[0.485, 0.456, 0.406],     \n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            key: torch.tensor(values[idx])\n",
    "            for key, values in self.encoded_captions.items()\n",
    "        }\n",
    "\n",
    "        item['image'] = self.transforms(self.images[idx])\n",
    "        item['caption'] = self.captions[idx]\n",
    "\n",
    "        return item\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "dataset = CLIPDataset(tokenizer)\n",
    "print(\"First sample:\", dataset[0])\n",
    "print(\"Dataset length:\", len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0afee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch.nn as nn\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode images to a fixed size vector\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(\n",
    "            'resnet34', pretrained=True, num_classes=0, global_pool=\"avg\"\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "image_encoder = ImageEncoder()\n",
    "image = dataset[0]['image'].unsqueeze(0)  # Add batch dimension\n",
    "image_embeddings = image_encoder(image)\n",
    "print(\"Image embedding shape:\", image_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700daba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "            \n",
    "        # we are using the CLS token hidden representation as the sentence's embedding\n",
    "        self.target_token_idx = 0\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        return last_hidden_state[:, self.target_token_idx, :]\n",
    "    \n",
    "text_encoder = TextEncoder()\n",
    "input_ids = dataset[0]['input_ids'].unsqueeze(0)  # Add batch dimension\n",
    "attention_mask = dataset[0]['attention_mask'].unsqueeze(0)  # Add batch dimension\n",
    "text_embedding_dim = text_encoder(input_ids, attention_mask)\n",
    "print(\"Text embedding shape:\", text_embedding_dim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6a52a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CLIPModule(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_embedding_dim=512,\n",
    "        text_embedding_dim=768,\n",
    "        embedding_dim=128,\n",
    "        lr=1e-4\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "\n",
    "        # freezing the image and text encoders\n",
    "        # This way I simply use the pre-trained weights of the encoders and only train the projections\n",
    "        for param in self.image_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.text_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.image_projection = nn.Parameter(torch.empty(image_embedding_dim, embedding_dim))\n",
    "        nn.init.normal_(self.image_projection, std=image_embedding_dim ** -0.5)\n",
    "        self.text_projection = nn.Parameter(torch.empty(text_embedding_dim, embedding_dim))\n",
    "        nn.init.normal_(self.text_projection, std=text_embedding_dim ** -0.5)\n",
    "\n",
    "        self.temperature = nn.Parameter(torch.empty(1))\n",
    "        nn.init.constant_(self.temperature, 0.0)  # Initialize temperature to 0.0\n",
    "\n",
    "        self.lr = lr\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Getting Image and Text Features\n",
    "        image_features = self.image_encoder(batch[\"image\"])\n",
    "        text_features = self.text_encoder(\n",
    "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "        # print(\"Image features shape:\", image_features.shape)\n",
    "        # print(\"Image features:\", image_features)\n",
    "        # print(\"Text features shape:\", text_features.shape)\n",
    "        # print(\"Text features:\", text_features)\n",
    "\n",
    "        # Getting Image and Text Embeddings (with same dimension)\n",
    "        image_embeddings = image_features @ self.image_projection\n",
    "        text_embeddings = text_features @ self.text_projection\n",
    "\n",
    "        # print(\"Image embeddings shape:\", image_embeddings.shape)\n",
    "        # print(\"Image embeddings:\", image_embeddings)\n",
    "        # print(\"Text embeddings shape:\", text_embeddings.shape)\n",
    "        # print(\"Text embeddings:\", text_embeddings)\n",
    "\n",
    "        # Calculating the Loss\n",
    "        logits = (image_embeddings @ text_embeddings.T) * torch.exp(self.temperature)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        logits = self(batch)\n",
    "        labels = torch.arange(len(batch[\"image\"]))\n",
    "\n",
    "        # print(\"Logits:\", logits)\n",
    "        # print(\"Labels:\", labels)\n",
    "\n",
    "        image_loss = F.cross_entropy(logits, labels, reduction='mean')\n",
    "        text_loss = F.cross_entropy(logits.T, labels, reduction='mean')\n",
    "        loss = (image_loss + text_loss) / 2\n",
    "\n",
    "        # print(\"Image loss:\", image_loss.item())\n",
    "        # print(\"Text loss:\", text_loss.item())\n",
    "        # print(\"Total loss:\", loss.item())\n",
    "\n",
    "        self.log(\"image_loss\", image_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"text_loss\", text_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        print(\"Training step loss:\", loss.item())\n",
    "\n",
    "        return loss\n",
    "    \n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=3,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    ")\n",
    "batch = next(iter(dataloader))\n",
    "clip_model = CLIPModule()\n",
    "clip_model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff44511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking dimensions for projection\n",
    "image = dataset[0]['image'].unsqueeze(0)  # Add batch dimension\n",
    "image_embeddings = image_encoder(image)\n",
    "\n",
    "image_projection = nn.Parameter(torch.empty(512, 128))\n",
    "print(\"Image embeddings shape:\", image_embeddings.shape)\n",
    "print(\"Image projection shape:\", image_projection.shape)\n",
    "final_embeddings = image_embeddings @ image_projection\n",
    "print(\"Image embedding shape:\", final_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc8ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=20,\n",
    "    accelerator=\"cpu\",\n",
    "    devices=1,\n",
    "    log_every_n_steps=1\n",
    ")\n",
    "clip_model = CLIPModule(lr=1e-3)\n",
    "trainer.fit(clip_model, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89efd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embeddings():\n",
    "    # get best model from the trainer\n",
    "    best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "    print(\"Best model path:\", best_model_path)\n",
    "    model = CLIPModule.load_from_checkpoint(best_model_path)\n",
    "    model.eval()\n",
    "\n",
    "    image_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            image_features = model.image_encoder(batch[\"image\"])\n",
    "            image_embeddings.append(image_features @ model.image_projection)\n",
    "    image_embeddings = torch.cat(image_embeddings, dim=0)\n",
    "    print(\"Image embeddings shape:\", image_embeddings.shape)\n",
    "    return image_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676826bf",
   "metadata": {},
   "source": [
    "# Masking out logits for duplicate captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19aa777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets pretend that the first and thrid captions are the same\n",
    "captions = [\"caption1\", \"caption2\", \"caption1\"]\n",
    "# we get logits like this\n",
    "logits = torch.Tensor([[1.0, 0.5, 1.0],\n",
    "                      [0.5, 1.0, 0.3],\n",
    "                      [1.0, 0.3, 1.0]])\n",
    "\n",
    "# then we want to maks out the logits at [1, 3] and [3, 1] like so:\n",
    "logits_desired_result = torch.Tensor([[1.0, 0.5, 0.0],\n",
    "                                      [0.5, 1.0, 0.3],\n",
    "                                      [0.0, 0.3, 1.0]])\n",
    "\n",
    "def get_mask(captions):\n",
    "    \"\"\"\n",
    "    Get a mask for the logits to set the values that are not on the diagonal of the captions to 0.0\n",
    "    \"\"\"\n",
    "    mask = torch.ones((len(captions), len(captions)))\n",
    "    for i in range(len(captions)):\n",
    "        for j in range(len(captions)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            if captions[i] == captions[j]:\n",
    "                mask[i, j] = 0.0\n",
    "    return mask\n",
    "\n",
    "# now I asked chatgpt for a more pytorch/numpy way to do this\n",
    "def get_mask_pytorch(captions):\n",
    "    # Step 1: Convert strings to unique indices\n",
    "    unique_captions = {caption: idx for idx, caption in enumerate(set(captions))}\n",
    "    caption_ids = torch.tensor([unique_captions[c] for c in captions])\n",
    "\n",
    "    # Step 2: Create comparison matrix\n",
    "    eq = caption_ids.unsqueeze(0) == caption_ids.unsqueeze(1)  # shape: (N, N)\n",
    "\n",
    "    # Step 3: Create mask\n",
    "    mask = torch.ones_like(eq, dtype=torch.float)\n",
    "    mask[eq & ~torch.eye(len(captions), dtype=torch.bool)] = 0.0\n",
    "\n",
    "    return mask\n",
    "\n",
    "mask = get_mask(captions)\n",
    "mask_pytorch = get_mask_pytorch(captions)\n",
    "logits_masked = logits * mask\n",
    "logits_masked_pytorch = logits * mask_pytorch\n",
    "\n",
    "print(\"Logits before masking:\\n\", logits)\n",
    "print(\"Mask:\\n\", mask)\n",
    "print(\"Logits after masking:\\n\", logits_masked)\n",
    "print(\"Logits after masking (pytorch):\\n\", logits_masked_pytorch)\n",
    "print(\"Desired result:\\n\", logits_desired_result)\n",
    "print(\"Are logits masked correctly?\", torch.allclose(logits_masked, logits_desired_result, atol=1e-6))\n",
    "print(\"Are logits masked correctly (pytorch)?\", torch.allclose(logits_masked_pytorch, logits_desired_result, atol=1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba71cd98",
   "metadata": {},
   "source": [
    "# Non Square Matrix Loss\n",
    "\n",
    "Problem: I have only 22 different captions in my pretraining dataset. That means captions will appear multiple times.\n",
    "\n",
    "Illustration: Lets consider the logits, so the cosine similarity between image and caption embeddings. The rows correspond to images and the columns to features\n",
    "\n",
    "What CE loss maximizes: X, the rest it minimizes\n",
    "\n",
    "|   |  caption1 | caption2  | caption1  | caption2  |\n",
    "|---|---|---|---|---|\n",
    "|  img1 |  XO |   | O  |   |\n",
    "| img2  |   | X  |   |   |\n",
    "| img3  |   |   | X  |   |\n",
    "| img4  |   |   |   | X  |\n",
    "\n",
    "The both O's are the same and get computed the same. The first one is maximized and the second one minimized, meaning that the loss tries to maximize and minimize the same thing at the same time.\n",
    "\n",
    "Idea: Remove duplicated columns and maximize accordingly\n",
    "\n",
    "|   |  caption1 | caption2 \n",
    "|---|---|---|\n",
    "|  img1 |  X |   |\n",
    "| img2  |   | X  |\n",
    "| img3  | X  |   |\n",
    "| img4  |   | X  |\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b56a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.randn(4, 3, 224, 224)  # Example images\n",
    "captions = [\"caption1\", \"caption2\", \"caption1\", \"caption1\"]\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "captions_tokenized = tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=True, max_length=20)\n",
    "\n",
    "image_encoder = ImageEncoder()\n",
    "image_embedding_dim = 512\n",
    "text_encoder = TextEncoder()\n",
    "text_embedding_dim = 768\n",
    "\n",
    "embedding_dim = 128  # Dimension for the final embeddings\n",
    "temperature = 0.0\n",
    "\n",
    "image_projection = nn.Parameter(torch.empty(image_embedding_dim, embedding_dim))\n",
    "nn.init.normal_(image_projection, std=image_embedding_dim ** -0.5)\n",
    "text_projection = nn.Parameter(torch.empty(text_embedding_dim, embedding_dim))\n",
    "nn.init.normal_(text_projection, std=text_embedding_dim ** -0.5)\n",
    "\n",
    "temperature = nn.Parameter(torch.empty(1))\n",
    "nn.init.constant_(temperature, 0.0)  # Initialize temperature to 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4941b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits(images, captions_tokenized):\n",
    "    image_features = image_encoder(images)\n",
    "    text_features = text_encoder(\n",
    "        input_ids=captions_tokenized[\"input_ids\"],\n",
    "        attention_mask=captions_tokenized[\"attention_mask\"]\n",
    "    )\n",
    "\n",
    "    image_embeddings = image_features @ image_projection\n",
    "    text_embeddings = text_features @ text_projection\n",
    "\n",
    "    # normalize embeddings\n",
    "    image_embeddings = F.normalize(image_embeddings, dim=-1)\n",
    "    text_embeddings = F.normalize(text_embeddings, dim=-1)\n",
    "\n",
    "    # print(\"Image embeddings shape:\", image_embeddings.shape)\n",
    "    # print(\"Text embeddings shape:\", text_embeddings.shape)\n",
    "    logits = (image_embeddings @ text_embeddings.T) * torch.exp(temperature)\n",
    "    return logits\n",
    "\n",
    "logits = get_logits(images, captions_tokenized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4d26fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_non_square_loss(logits, captions):\n",
    "    # get duplicate captions\n",
    "    _, caption_ids = np.unique(captions, return_inverse=True) # get the uniqueness by captions and not the embeddings, since they might actually differ due to dropout during training\n",
    "    # turn into torch tensor\n",
    "    caption_ids = torch.tensor(caption_ids, dtype=torch.int64)\n",
    "\n",
    "    # there are multiple \"classes\" now to which we should maximize, we get them by getting the indices of the unique captions\n",
    "    unique_vals = torch.unique(caption_ids)\n",
    "    class_indices = [(caption_ids == val).nonzero(as_tuple=True)[0].tolist() for val in unique_vals]\n",
    "\n",
    "    # given the unique caption ids, remove duplicate columns\n",
    "    unique_ids = torch.unique(caption_ids, return_inverse=False, return_counts=False, sorted=False)\n",
    "    # For each unique id, get the FIRST index where it occurs\n",
    "    selected_indices = torch.stack([torch.where(caption_ids == uid)[0][0] for uid in unique_ids])\n",
    "    # Select logits\n",
    "    selected_logits = logits[:, selected_indices]\n",
    "    # print(\"Selected logits shape:\", selected_logits.shape)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    labels = torch.zeros_like(selected_logits)\n",
    "    for class_id, indices in enumerate(class_indices):\n",
    "        labels[indices, class_id] = 1.0\n",
    "\n",
    "    # print(\"Labels :\", labels)\n",
    "\n",
    "    loss_img = criterion(selected_logits, labels)\n",
    "    loss_text = criterion(selected_logits.T, labels.T)\n",
    "\n",
    "    print('loss_img (cross entropy loss for image):', loss_img.item())\n",
    "    print('loss_text (bce for image):', torch.nn.BCEWithLogitsLoss()(selected_logits, labels).item())\n",
    "\n",
    "    print(\"Loss image:\", loss_img.item())\n",
    "    print(\"Loss text:\", loss_text.item())\n",
    "\n",
    "    loss = (loss_img + loss_text) / 2\n",
    "\n",
    "    return loss\n",
    "\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "print('Logits:', logits)\n",
    "compute_non_square_loss(logits, captions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239953e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logits:\\n\", logits)\n",
    "\n",
    "sigmoid_0 = torch.nn.functional.sigmoid(logits)\n",
    "\n",
    "print(\"sigmoid along dim 0:\\n\", sigmoid_0)\n",
    "# print(\"sigmoid along dim 1:\\n\", sigmoid_1)\n",
    "\n",
    "compute_non_square_loss(logits, captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e2347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_normal_loss(logits):\n",
    "    labels = torch.arange(len(logits))\n",
    "\n",
    "    image_loss = F.cross_entropy(logits, labels, reduction='mean')\n",
    "    text_loss = F.cross_entropy(logits.T, labels, reduction='mean')\n",
    "\n",
    "    print(\"Image loss:\", image_loss.item())\n",
    "    print(\"Text loss:\", text_loss.item())\n",
    "\n",
    "    loss = (image_loss + text_loss) / 2\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1948390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_masked_loss(logits, captions):\n",
    "    mask = get_mask_pytorch(captions)\n",
    "    print(\"Mask:\", mask)\n",
    "    logits_masked = logits * mask\n",
    "    loss = compute_normal_loss(logits_masked)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7086652",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_non_square = compute_non_square_loss(logits, captions)\n",
    "loss_normal = compute_normal_loss(logits)\n",
    "print(\"Computed normal loss:\", loss_normal.item())\n",
    "print(\"Computed non square loss:\", loss_non_square.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfca3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check, if no captions are duplicated\n",
    "images = torch.randn(4, 3, 224, 224)  # Example images\n",
    "captions_unique = [\"caption1\", \"caption2\", \"caption3\", \"caption4\"]\n",
    "captions_tokenized_unique = tokenizer(captions_unique, return_tensors=\"pt\", padding=True, truncation=True, max_length=20)\n",
    "\n",
    "logits_unique = get_logits(images, captions_tokenized_unique)\n",
    "loss_normal_unique = compute_normal_loss(logits_unique)\n",
    "loss_masked_unique = compute_masked_loss(logits_unique, captions_unique)\n",
    "loss_non_square_unique = compute_non_square_loss(logits_unique, captions_unique)\n",
    "\n",
    "print(\"Computed normal loss (unique captions):\", loss_normal_unique.item())\n",
    "print(\"Computed masked loss (unique captions):\", loss_masked_unique.item())\n",
    "print(\"Computed non square loss (unique captions):\", loss_non_square_unique.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03ebc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One caption duplicated\n",
    "images = torch.randn(4, 3, 224, 224)  # Example images\n",
    "captions_unique = [\"caption1\", \"caption2\", \"caption3\", \"caption1\"]\n",
    "captions_tokenized_unique = tokenizer(captions_unique, return_tensors=\"pt\", padding=True, truncation=True, max_length=20)\n",
    "\n",
    "logits_unique = get_logits(images, captions_tokenized_unique)\n",
    "loss_normal_unique = compute_normal_loss(logits_unique)\n",
    "loss_masked_unique = compute_masked_loss(logits_unique, captions_unique)\n",
    "loss_non_square_unique = compute_non_square_loss(logits_unique, captions_unique)\n",
    "\n",
    "print(\"Computed normal loss (unique captions):\", loss_normal_unique.item())\n",
    "print(\"Computed masked loss (unique captions):\", loss_masked_unique.item())\n",
    "print(\"Computed non square loss (unique captions):\", loss_non_square_unique.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610d6bb5",
   "metadata": {},
   "source": [
    "## Precision @ k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf06e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k_on_image_embeddings(image_embeddings, labels, k=3):\n",
    "    assert k + 1 <= image_embeddings.shape[0], \"k+1 must be less than or equal to the batch size\"\n",
    "\n",
    "    # compute the cosine similarity between all image embedding pairs\n",
    "    image_embeddings = torch.nn.functional.normalize(image_embeddings)\n",
    "    similarity_matrix = image_embeddings @ image_embeddings.T  # [batch_size, batch_size]\n",
    "    print(\"Similarity matrix:\", similarity_matrix)\n",
    "    # get the top k indices for each image embedding\n",
    "    top_k_indices = similarity_matrix.topk(k=k+1, dim=1).indices  # [batch_size, k]\n",
    "    # remove the first index, which is the image itself (self-similarity)\n",
    "    print(\"Top k indices:\", top_k_indices)\n",
    "    top_k_indices = top_k_indices[:, 1:]  # [batch_size, k\n",
    "    # check if the labels of the top k indices match the labels of the current image embedding\n",
    "    correct_predictions = (labels.unsqueeze(1) == labels[top_k_indices]).sum(dim=1)  # [batch_size]\n",
    "    # compute the precision at k\n",
    "    precision_at_k = correct_predictions.float() / k  # [batch_size]\n",
    "    return precision_at_k.mean()  # return the mean precision at k over the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46433e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeddings = torch.Tensor([[1, 1], [1, 1.1],  [2, 1], [3, 1]])\n",
    "labels = torch.Tensor([0, 0, 1, 1])\n",
    "precision_at_k_on_image_embeddings(image_embeddings, labels, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee02e746",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_embeddings_normalized_1 = F.normalize(image_embeddings)\n",
    "cosine_similarities_1 = image_embeddings_normalized_1 @ image_embeddings_normalized_1.T\n",
    "print(\"Cosine similarities (normalized):\", cosine_similarities_1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
