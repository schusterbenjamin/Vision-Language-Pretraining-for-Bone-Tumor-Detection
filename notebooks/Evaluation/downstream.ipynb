{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "433dd5b8",
   "metadata": {},
   "source": [
    "# Downstream Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fa971f",
   "metadata": {},
   "source": [
    "Used to generate plots for downstream performance evaluation.\n",
    "You need to provide a `.csv`-file with the following columns: `[level,group,fold,metric,value]`.\n",
    "For example like this:\n",
    "\n",
    "```csv\n",
    "level,group,fold,metric,value\n",
    "overall,overall,0,accuracy,0.6460431654676259\n",
    "overall,overall,0,balanced_accuracy,0.6463224565592254\n",
    "overall,overall,0,roc_auc,0.7061986006101605\n",
    "overall,overall,0,precision,0.663768115942029\n",
    "overall,overall,0,recall,0.637883008356546\n",
    "overall,overall,0,f1_score,0.6505681818181818\n",
    "dataset,INTERNAL,0,accuracy,0.6287051482059283\n",
    "dataset,INTERNAL,0,balanced_accuracy,0.6272424598511555\n",
    "dataset,INTERNAL,0,roc_auc,0.6942322757540149\n",
    "dataset,INTERNAL,0,precision,0.6578171091445427\n",
    "dataset,INTERNAL,0,recall,0.6463768115942029\n",
    "dataset,INTERNAL,0,f1_score,0.652046783625731\n",
    "dataset,BTXRD,0,accuracy,0.6608811748998665\n",
    "dataset,BTXRD,0,balanced_accuracy,0.6607580856768012\n",
    "dataset,BTXRD,0,roc_auc,0.7119032000456335\n",
    "dataset,BTXRD,0,precision,0.6695156695156695\n",
    "dataset,BTXRD,0,recall,0.6300268096514745\n",
    "dataset,BTXRD,0,f1_score,0.649171270718232\n",
    "entity,Enchondrom,0,accuracy,0.6296296296296297\n",
    "...\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decbce58",
   "metadata": {},
   "source": [
    "## Setup Functions and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a469583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e97ecc",
   "metadata": {},
   "source": [
    "### Get Computed Evaluation\n",
    "\n",
    "These are precomputed metrics on the overall dataset and on each group for each metadata attribute (age, gender, anatomy-site) (as shown above).\n",
    "This is nice, so, we don't have to compute the metrics in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eef728f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pd dataframe\n",
    "# file = \"../../evaluation/baseline/only_imaging_baseline_resnet34.csv\" # ResNet34 Baseline Only Imaging\n",
    "# file = \"../../evaluation/baseline/imaging_and_clinical_baseline_resnet34.csv\" # ResNet34 Baseline Imaging+Clinical\n",
    "# file = \"../../evaluation/baseline/only_imaging_baseline_nest_small.csv\" # Nest Small Baseline Only Imaging\n",
    "# file = \"../../evaluation/baseline/imaging_and_clinical_baseline_nest_small.csv\" # Nest Small Baseline Imaging+Clinical\n",
    "# file = \"../../evaluation/baseline_pretrained/only_imaging_pretrained_baseline_resnet50.csv\" # ResNet50 Pretrained Baseline Only Imaging\n",
    "# file = \"../../evaluation/baseline_pretrained/imaging_and_clinical_pretrained_baseline_resnet50.csv\" # ResNet50 Pretrained Baseline Imaging+Clinical\n",
    "# file = \"../../evaluation/vlp/linear_probe_only_imaging_resnet34.csv\" # ResNet34 VLP Linear Probe Only Imaging\n",
    "# file =  \"../../evaluation/finetune/only_imaging_finetune_resnet34.csv\" # ResNet34 Finetune Only Imaging\n",
    "file = \"../../evaluation/finetune/imaging_and_clinical_finetune_resnet34.csv\" # ResNet34 Finetune Imaging+Clinical\n",
    "\n",
    "df = pd.read_csv(file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4193cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of folds\n",
    "number_of_folds = np.unique(df[\"fold\"]).shape[0]\n",
    "print(f\"Number of folds: {number_of_folds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea1f6b8",
   "metadata": {},
   "source": [
    "### Get Raw Predictions\n",
    "\n",
    "Compared to the already computed metrics, we also get the raw predictions. We could compute the metrics from these raw predictions, but that's already done by the evaluation script.\n",
    "Nevertheless, for the creation of a confusion matrix the raw predictions are needed, that's why we load them here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c23743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I only saved predictions for overal two best models: pretrained baseline resnet50 imaging+clinical and finetuned resnet34 imaging+clinical\n",
    "\n",
    "# predictions_file = \"../../predictions/baseline_pretrained/imaging_and_clinical_pretrained_baseline_resnet50/predictions_fold_0.csv\" # best fold on val (we should not decide based on test set performance for obvious reasons)\n",
    "predictions_file = \"../../predictions/finetune/imaging_and_clinical_finetune_resnet34/predictions_fold_0.csv\" # best fold on val (we should not decide based on test set performance for obvious reasons)\n",
    "\n",
    "pred_df = pd.read_csv(predictions_file)\n",
    "# get actual predictions from probs\n",
    "pred_df['pred'] = pred_df['prob'].apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6683b1",
   "metadata": {},
   "source": [
    "### Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c73014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df):\n",
    "    # remove rows, where value is NaN\n",
    "    clean_df = df[~df[\"value\"].isna()]\n",
    "    print(f\"Number of rows removed: {len(df) - len(clean_df)}\")\n",
    "    return clean_df\n",
    "\n",
    "clean_df = clean(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e318e940",
   "metadata": {},
   "source": [
    "#### Get mean and std over folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c6caf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def average_over_folds(clean_df):\n",
    "    number_of_folds = np.unique(clean_df[\"fold\"]).shape[0]\n",
    "    avg_df = clean_df.groupby([\"level\", \"group\", \"metric\"]).agg({'value': ['mean', 'std']}).reset_index()\n",
    "\n",
    "    assert len(avg_df) == len(clean_df) / number_of_folds, f\"Expected {len(clean_df) / number_of_folds} rows, but got {len(avg_df)} rows.\"\n",
    "\n",
    "    return avg_df\n",
    "\n",
    "avg_df = average_over_folds(clean_df)\n",
    "avg_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88428399",
   "metadata": {},
   "source": [
    "### Get occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30f4362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get occurences of different attributes\n",
    "occurences_df = pd.read_csv(\"../../evaluation/occurrences.csv\")\n",
    "occurences_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1605a52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize counts, s.t. for each group its sums up to 1\n",
    "# just get the total count over the dataset attribute, it is the same total count for each attribute\n",
    "total_count = occurences_df[occurences_df[\"attribute\"] == \"dataset\"][\"count\"].sum()\n",
    "occurences_df[\"normalized_count\"] = occurences_df[\"count\"] / total_count\n",
    "occurences_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d179a92",
   "metadata": {},
   "source": [
    "### Metadata\n",
    "\n",
    "This is similar to occurences. But different in two ways:\n",
    "-  occurences has already computed values for each group (e.g. dataset: INTERNAL, dataset: BTXRD, entity: ostechondroma, etc.), wheras metadata is just a entry for every sample with the metadata\n",
    "-  this metadata df is from the entire downstream data, wheras the occurence is just based on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0198ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = pd.read_csv(\"../../visualizations/data/downstream/metadata.csv\")\n",
    "metadata_df = metadata_df.drop(['Unnamed: 0'], axis=1)\n",
    "metadata_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9412c3c0",
   "metadata": {},
   "source": [
    "## Single Experiment Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d286745",
   "metadata": {},
   "source": [
    "### Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d13453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the overal metrics with mean and std\n",
    "print(\"Overall metrics:\")\n",
    "overall_metrics = avg_df[avg_df[\"level\"] == \"overall\"]\n",
    "print(overall_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466f422b",
   "metadata": {},
   "source": [
    "### Plotting Function Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66512d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_per_group(avg_dfs, group, group_name=None, occurences_df=None, order_by_occurrence=False, exclude_most_common=False, df_titles=None, show_values=True):\n",
    "    \"\"\"\n",
    "    Plot metrics per group for one or multiple dataframes.\n",
    "    \n",
    "    Args:\n",
    "        avg_dfs: Single dataframe or list of dataframes to plot (each representing a different experiment)\n",
    "        group: Group to plot metrics for (e.g., 'entity', 'anatomy_site')\n",
    "        group_name: Optional display name for the group\n",
    "        occurences_df: Dataframe with occurrence data (optional)\n",
    "        order_by_occurrence: Whether to order bars by occurrence (requires occurences_df)\n",
    "        exclude_most_common: Whether to exclude most common group from color normalization\n",
    "        df_titles: Optional list of titles for each experiment (if multiple)\n",
    "    \"\"\"\n",
    "    # Handle single dataframe case\n",
    "    if not isinstance(avg_dfs, list):\n",
    "        avg_dfs = [avg_dfs]\n",
    "    \n",
    "    # Setup df_titles if not provided\n",
    "    n_dfs = len(avg_dfs)\n",
    "    if df_titles is None:\n",
    "        df_titles = [f\"Experiment {i+1}\" for i in range(n_dfs)]\n",
    "    else:\n",
    "        assert len(df_titles) == n_dfs, \"Number of titles must match number of experiments\"\n",
    "\n",
    "    if order_by_occurrence:\n",
    "        assert occurences_df is not None, \"To order by occurrence, occurrence data must be provided\"\n",
    "    \n",
    "    if exclude_most_common:\n",
    "        assert occurences_df is not None, \"To exclude most common, occurrence data must be provided\"\n",
    "\n",
    "    # Get unique group values across all dataframes first\n",
    "    all_group_values = set()\n",
    "    for avg_df in avg_dfs:\n",
    "        group_metrics_df = avg_df[avg_df[\"level\"] == group]\n",
    "        group_values = group_metrics_df[\"group\"].unique()\n",
    "        all_group_values.update(group_values)\n",
    "    \n",
    "    # Get metrics data for each dataframe\n",
    "    all_metrics_dfs = []\n",
    "    for df_idx, avg_df in enumerate(avg_dfs):\n",
    "        # Filter for group metrics\n",
    "        metrics_df = avg_df[avg_df[\"level\"] == group]\n",
    "        metrics_df = metrics_df.copy()\n",
    "        metrics_df[\"df_idx\"] = df_idx  # Add source dataframe index\n",
    "        metrics_df[\"df_title\"] = df_titles[df_idx]  # Add source dataframe title\n",
    "        all_metrics_dfs.append(metrics_df)\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_metrics_df = pd.concat(all_metrics_dfs, ignore_index=True)\n",
    "    \n",
    "    # Handle occurrence data\n",
    "    use_heatmap = False\n",
    "    order = None\n",
    "    most_common_group = None\n",
    "    \n",
    "    if occurences_df is not None:\n",
    "        # Get occurrence data for this group\n",
    "        group_occurrences = occurences_df[occurences_df[\"attribute\"] == group]\n",
    "        \n",
    "        # Only proceed with heatmap if we have matching occurrence data\n",
    "        if len(group_occurrences) > 0:\n",
    "            use_heatmap = True\n",
    "            \n",
    "            # Use normalized_count if available, otherwise use count\n",
    "            count_col = \"normalized_count\" if \"normalized_count\" in group_occurrences.columns else \"count\"\n",
    "            \n",
    "            # Find the most common group\n",
    "            if exclude_most_common:\n",
    "                most_common_group = group_occurrences.loc[group_occurrences[count_col].idxmax(), \"value\"]\n",
    "            \n",
    "            # Create a mapping of group values to counts\n",
    "            count_map = dict(zip(group_occurrences[\"value\"], group_occurrences[count_col]))\n",
    "            \n",
    "            # Add count information to metrics_df\n",
    "            combined_metrics_df[\"count\"] = combined_metrics_df[\"group\"].map(count_map)\n",
    "            \n",
    "            # Set order based on occurrence if requested\n",
    "            if order_by_occurrence:\n",
    "                # Sort group occurrences by count for ordering\n",
    "                order = group_occurrences.sort_values(by=count_col)[\"value\"].tolist()\n",
    "    \n",
    "    # Get unique metrics and unique group values across all dataframes\n",
    "    metrics = combined_metrics_df[\"metric\"].unique()\n",
    "    \n",
    "    # Create a figure for each metric\n",
    "    plots = {}\n",
    "    for metric in metrics:\n",
    "        # Filter for current metric across all dataframes\n",
    "        metric_data = combined_metrics_df[combined_metrics_df[\"metric\"] == metric]\n",
    "        \n",
    "        # Create plot\n",
    "        plt.figure(figsize=(14, 8)).set_dpi(600)\n",
    "        \n",
    "        # Create palette for heatmap if needed\n",
    "        palette = None\n",
    "        if use_heatmap:\n",
    "            if \"count\" in metric_data.columns:\n",
    "                # Prepare data for normalization\n",
    "                if exclude_most_common:\n",
    "                    # Create a mask for non-most-common groups\n",
    "                    non_most_common_mask = metric_data[\"group\"] != most_common_group\n",
    "                    \n",
    "                    # Get min and max counts only from non-most-common groups\n",
    "                    min_count = metric_data.loc[non_most_common_mask, \"count\"].min()\n",
    "                    max_count = metric_data.loc[non_most_common_mask, \"count\"].max()\n",
    "                else:\n",
    "                    min_count = metric_data[\"count\"].min()\n",
    "                    max_count = metric_data[\"count\"].max()\n",
    "                \n",
    "                # Map each group value to a color based on its count\n",
    "                norm = plt.Normalize(min_count, max_count)\n",
    "                cmap = plt.cm.coolwarm\n",
    "                \n",
    "                if n_dfs > 1:\n",
    "                    # For multiple dataframes, the palette needs to map to df_titles\n",
    "                    # Create a base color for each group\n",
    "                    group_colors = {}\n",
    "                    for g in metric_data[\"group\"].unique():\n",
    "                        if exclude_most_common and g == most_common_group:\n",
    "                            group_colors[g] = 'black'\n",
    "                        else:\n",
    "                            g_count = metric_data[metric_data[\"group\"] == g][\"count\"].iloc[0]\n",
    "                            group_colors[g] = cmap(norm(g_count))\n",
    "                    \n",
    "                    # Now create a palette that maps each df_title to its appropriate color\n",
    "                    # For seaborn's catplot with hue, we need the palette to be keyed by the hue values\n",
    "                    palette = {}\n",
    "                    for title in df_titles:\n",
    "                        for g in metric_data[\"group\"].unique():\n",
    "                            # Create a shade of the group color for each dataset\n",
    "                            base_color = group_colors[g]\n",
    "                            # We use the same color for all datasets for each group\n",
    "                            # as we're coloring by group occurrence, not by dataset\n",
    "                            palette[title] = base_color\n",
    "                else:\n",
    "                    # Single dataframe case - map group values to colors\n",
    "                    palette = {}\n",
    "                    for g in metric_data[\"group\"].unique():\n",
    "                        if exclude_most_common and g == most_common_group:\n",
    "                            palette[g] = 'black'\n",
    "                        else:\n",
    "                            g_count = metric_data[metric_data[\"group\"] == g][\"count\"].iloc[0]\n",
    "                            palette[g] = cmap(norm(g_count))\n",
    "        \n",
    "        # Create the plot\n",
    "        if n_dfs > 1:\n",
    "            # For multiple experiments, use a different approach - use FacetGrid\n",
    "            # Since there are issues with palette mapping in catplot with hue\n",
    "            # We'll create a barplot on each position manually\n",
    "            plt.figure(figsize=(14, 8))\n",
    "            ax = plt.gca()\n",
    "            \n",
    "            # Get the unique groups and sort them if order is specified\n",
    "            if order is not None:\n",
    "                # Make sure all groups from all dataframes are included\n",
    "                unique_groups = pd.Series(order).loc[pd.Series(order).isin(all_group_values)].tolist()\n",
    "            else:\n",
    "                unique_groups = sorted(list(all_group_values))\n",
    "            \n",
    "            # Define bar width and positions with more spacing\n",
    "            bar_width = 0.7 / n_dfs  # Reduced from 0.8 to create more space between bars\n",
    "            group_positions = np.arange(len(unique_groups))\n",
    "            group_spacing = 0.05  # Add spacing between bars within a group\n",
    "            \n",
    "            # Define hatch patterns for different experiments - using clearly visible patterns\n",
    "            hatch_patterns = ['////', '....', '\\\\\\\\\\\\\\\\', 'xxxx', '++++', 'oooo', '||||']\n",
    "            \n",
    "            # Create empty list to store legend handles\n",
    "            legend_handles = []\n",
    "            \n",
    "            # For each experiment, add a set of bars\n",
    "            for df_idx, df_title in enumerate(df_titles):\n",
    "                # Filter data for this experiment\n",
    "                df_data = metric_data[metric_data[\"df_title\"] == df_title]\n",
    "                \n",
    "                # Prepare heights, errors, and positions for each group\n",
    "                heights = []\n",
    "                errors = []\n",
    "                positions = []\n",
    "                \n",
    "                for i, g in enumerate(unique_groups):\n",
    "                    group_data = df_data[df_data[\"group\"] == g]\n",
    "                    if not group_data.empty:\n",
    "                        heights.append(group_data[(\"value\", \"mean\")].iloc[0])\n",
    "                        errors.append(group_data[(\"value\", \"std\")].iloc[0])\n",
    "                        positions.append(i)\n",
    "                # Don't add this group if it's missing from this dataframe\n",
    "            \n",
    "                # Calculate bar positions with spacing\n",
    "                bar_positions = np.array(positions) + (df_idx - n_dfs/2 + 0.5) * (bar_width + group_spacing)\n",
    "                \n",
    "                # Determine colors for each bar\n",
    "                if use_heatmap:\n",
    "                    colors = []\n",
    "                    for i, pos in enumerate(positions):\n",
    "                        g = unique_groups[pos]  # Get the group value using integer index\n",
    "                        if exclude_most_common and g == most_common_group:\n",
    "                            colors.append('black')\n",
    "                        else:\n",
    "                            g_count = df_data[df_data[\"group\"] == g][\"count\"].iloc[0]\n",
    "                            colors.append(cmap(norm(g_count)))\n",
    "                else:\n",
    "                    # Use a consistent color scheme (not different for each experiment)\n",
    "                    colors = [plt.cm.coolwarm(0.5)] * len(heights)  # Neutral color\n",
    "                \n",
    "                # Define different hatch patterns for different experiments\n",
    "                hatch = hatch_patterns[df_idx % len(hatch_patterns)]\n",
    "                \n",
    "                # Draw the bars with visible hatches but no visible borders\n",
    "                bars = ax.bar(\n",
    "                    bar_positions, \n",
    "                    heights, \n",
    "                    width=bar_width,\n",
    "                    yerr=errors,\n",
    "                    color=colors,\n",
    "                    alpha=0.8,\n",
    "                    capsize=5,\n",
    "                    label=df_title,\n",
    "                    hatch=hatch,  # Add hatch pattern to distinguish experiments\n",
    "                    edgecolor=\"#454545\",  # Use same color as fill for the edge (invisible border but hatch shows)\n",
    "                    linewidth=0.5  # Thin line for hatch visibility\n",
    "                )\n",
    "                \n",
    "                # Create a separate patch for the legend with clearly visible hatches\n",
    "                \n",
    "                # Create a rectangle for the legend with prominent hatches\n",
    "                legend_patch = plt.Rectangle(\n",
    "                    (0, 0), 1, 1, \n",
    "                    fill=True,\n",
    "                    hatch=hatch*2,  # Double the hatch density for better visibility\n",
    "                    label=df_title,\n",
    "                    edgecolor='black',  # Add black border for better visibility\n",
    "                    linewidth=0.5,  # Thin border line\n",
    "                    facecolor='lightgray'  # Light gray background makes hatches more visible\n",
    "                )\n",
    "                legend_handles.append(legend_patch)\n",
    "                \n",
    "                # Add value labels\n",
    "                if show_values:\n",
    "                    for bar, height in zip(bars, heights):\n",
    "                        ax.text(\n",
    "                            bar.get_x() + bar.get_width()/2.,\n",
    "                            height + 0.01,\n",
    "                            f'{height:.3f}',\n",
    "                            ha='center',\n",
    "                            va='bottom',\n",
    "                            fontsize=9\n",
    "                        )\n",
    "            \n",
    "            # Add the legend with clear, visible rectangles\n",
    "            legend = plt.legend(\n",
    "                handles=legend_handles, \n",
    "                title=\"Experiments\", \n",
    "                loc='lower right',\n",
    "                framealpha=1.0,  # Solid legend background\n",
    "                handlelength=1.8,  # Legend rectangle width\n",
    "                handleheight=1.2,  # Legend rectangle height\n",
    "                labelspacing=0.6   # Add more space between legend items\n",
    "            )\n",
    "            \n",
    "            # Force drawing of the legend to ensure hatches are rendered properly\n",
    "            plt.draw()\n",
    "            \n",
    "            # Access legend patches directly from the legend_handles we created\n",
    "            # This avoids using the legendHandles attribute which can be problematic\n",
    "            \n",
    "            # Explicitly set the renderer for the legend to make sure hatches are drawn\n",
    "            plt.draw()\n",
    "            \n",
    "            # Set x-axis ticks and labels with rotation to prevent overlap\n",
    "            ax.set_xticks(group_positions)\n",
    "            ax.set_xticklabels(unique_groups, rotation=45, ha='right')\n",
    "            \n",
    "            # Check if x-tick labels would overlap and adjust if needed\n",
    "            fig = plt.gcf()\n",
    "            fig.canvas.draw()\n",
    "            tick_labels = ax.get_xticklabels()\n",
    "            if len(tick_labels) > 0:\n",
    "                # Get bounding boxes of tick labels\n",
    "                bboxes = [label.get_window_extent() for label in tick_labels]\n",
    "                # Check if any labels overlap\n",
    "                overlap = False\n",
    "                for i in range(len(bboxes)-1):\n",
    "                    if bboxes[i].x1 > bboxes[i+1].x0:\n",
    "                        overlap = True\n",
    "                        break\n",
    "                # If overlap detected, increase rotation angle\n",
    "                if overlap:\n",
    "                    ax.set_xticklabels(unique_groups, rotation=60, ha='right')\n",
    "        \n",
    "        else:\n",
    "            # Single dataframe case - use barplot without visible borders\n",
    "            ax = sns.barplot(\n",
    "                x=\"group\",\n",
    "                y=(\"value\", \"mean\"),\n",
    "                data=metric_data,\n",
    "                order=order,\n",
    "                palette=palette,\n",
    "                capsize=5,\n",
    "                alpha=0.8,\n",
    "                edgecolor=\"w\",  # White border to blend with background\n",
    "                linewidth=0.1  # Very thin line\n",
    "            )\n",
    "            \n",
    "            # Add error bars manually\n",
    "            for i, bar in enumerate(ax.patches):\n",
    "                # Get corresponding data point\n",
    "                if order is not None:\n",
    "                    group_val = order[i]\n",
    "                    idx = metric_data[metric_data[\"group\"] == group_val].index[0]\n",
    "                else:\n",
    "                    group_val = metric_data[\"group\"].unique()[i]\n",
    "                    idx = metric_data[metric_data[\"group\"] == group_val].index[0]\n",
    "                \n",
    "                # Get error value and bar height\n",
    "                err = metric_data.iloc[metric_data.index.get_indexer([idx])[0]][(\"value\", \"std\")]\n",
    "                height = bar.get_height()\n",
    "                \n",
    "                # Add error bar\n",
    "                ax.errorbar(\n",
    "                    x=i,\n",
    "                    y=height,\n",
    "                    yerr=err,\n",
    "                    fmt='none',\n",
    "                    ecolor='black',\n",
    "                    capsize=5\n",
    "                )\n",
    "                \n",
    "                # Add value label\n",
    "                if show_values:\n",
    "                    ax.text(\n",
    "                        bar.get_x() + bar.get_width()/2.,\n",
    "                        height + 0.01,\n",
    "                        f'{height:.3f}',\n",
    "                        ha='center',\n",
    "                        va='bottom',\n",
    "                        fontsize=10\n",
    "                    )\n",
    "            \n",
    "            # Set x-axis tick labels with no rotation by default\n",
    "            plt.xticks(rotation=0, ha='center')\n",
    "            \n",
    "            # Check if x-tick labels would overlap and adjust if needed\n",
    "            fig = plt.gcf()\n",
    "            fig.canvas.draw()\n",
    "            tick_labels = ax.get_xticklabels()\n",
    "            if len(tick_labels) > 0:\n",
    "                # Get bounding boxes of tick labels\n",
    "                bboxes = [label.get_window_extent() for label in tick_labels]\n",
    "                # Check if any labels overlap\n",
    "                overlap = False\n",
    "                for i in range(len(bboxes)-1):\n",
    "                    if bboxes[i].x1 > bboxes[i+1].x0:\n",
    "                        overlap = True\n",
    "                        break\n",
    "                # If overlap detected, use 45 degree rotation\n",
    "                if overlap:\n",
    "                    plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "        # Customize the plot\n",
    "        title = f'{metric.replace(\"_\", \" \").title() if metric != 'roc_auc' else 'AUROC'} by {group_name if group_name else group}'\n",
    "            \n",
    "        plt.title(title, fontsize=16)\n",
    "        plt.ylabel((metric.replace(\"_\", \" \").title() if metric != 'roc_auc' else 'AUROC'), fontsize=14)\n",
    "        plt.xlabel(f'{group_name if group_name else group}', fontsize=14)\n",
    "        \n",
    "        # Set y-axis limit\n",
    "        max_height = metric_data[(\"value\", \"mean\")].max()\n",
    "        plt.ylim(0, max(1.0, max_height * 1.15))\n",
    "        \n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Add color bar for heatmap\n",
    "        if use_heatmap and \"count\" in metric_data.columns:\n",
    "            sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "            sm.set_array([])\n",
    "            cbar = plt.colorbar(sm, ax=ax)\n",
    "            count_label = \"Normalized occurrence\" if \"normalized_count\" in group_occurrences.columns else \"Count\"\n",
    "            if exclude_most_common:\n",
    "                cbar.set_label(f'{count_label} of {group_name if group_name else group} (except {most_common_group})', fontsize=12)\n",
    "            else:\n",
    "                cbar.set_label(f'{count_label} of {group_name if group_name else group}', fontsize=12)\n",
    "        \n",
    "        # Add note about error bars centered under the plot\n",
    "        ax.text(0.5, -0.15, 'Error bars represent standard deviation across folds', \n",
    "                transform=ax.transAxes, ha='center', va='top', fontsize=10, alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plot_to_return = plt.gcf()\n",
    "        plots[metric] = plot_to_return\n",
    "        plt.show()\n",
    "    return plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a852c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mean_and_std_over_group(df, group):\n",
    "    metrics_df = avg_df[avg_df[\"level\"] == group]\n",
    "\n",
    "    metrics = metrics_df[\"metric\"].unique()\n",
    "    for metric in metrics:\n",
    "        print(f\"\\nMetric: {metric}\")\n",
    "\n",
    "        metric_by_group = metrics_df[metrics_df[\"metric\"] == metric]\n",
    "\n",
    "        # Calculate mean and std of metric across all group types\n",
    "        mean_metric = metric_by_group[(\"value\", \"mean\")].mean()\n",
    "        std_metric = metric_by_group[(\"value\", \"mean\")].std()\n",
    "\n",
    "        print(f\"Mean ± std {metric} across all {group} types: {mean_metric:.4f} ± {std_metric:.4f}\")\n",
    "\n",
    "        # Calculate median metric\n",
    "        median_metric = metric_by_group[(\"value\", \"mean\")].median()\n",
    "        print(f\"Median {metric} across all {group} types: {median_metric:.4f}\")\n",
    "\n",
    "        # # Get min and max metric with corresponding group types\n",
    "        min_metric_idx = metric_by_group[(\"value\", \"mean\")].idxmin()\n",
    "        max_metric_idx = metric_by_group[(\"value\", \"mean\")].idxmax()\n",
    "\n",
    "        min_group = metric_by_group.loc[min_metric_idx, \"group\"]\n",
    "        min_value = metric_by_group.loc[min_metric_idx, (\"value\", \"mean\")]\n",
    "\n",
    "        max_group = metric_by_group.loc[max_metric_idx, \"group\"]\n",
    "        max_value = metric_by_group.loc[max_metric_idx, (\"value\", \"mean\")]\n",
    "\n",
    "        print(f\"Lowest {metric}: {min_value:.4f} ({min_group})\")\n",
    "        print(f\"Highest {metric}: {max_value:.4f} ({max_group})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15655d1",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afefeff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=['No Tumor', 'Tumor']\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(pred_df['tumor'], pred_df['pred'])\n",
    "disp = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=labels)\n",
    "disp = disp.plot(cmap=plt.cm.Blues)\n",
    "disp.figure_.set_dpi(500)\n",
    "\n",
    "plt.savefig(\"../../visualizations/data/downstream/confusion_matrix.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0375af7e",
   "metadata": {},
   "source": [
    "### Per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77643210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for dataset metrics\n",
    "plot_metrics_per_group(avg_df, \"dataset\", group_name=\"Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac04314b",
   "metadata": {},
   "source": [
    "### Per Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097c0c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_per_group(avg_df, \"entity\", group_name=\"Entity\", occurences_df=occurences_df, order_by_occurrence=True, exclude_most_common=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed37871",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mean_and_std_over_group(avg_df, \"entity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715ceace",
   "metadata": {},
   "source": [
    "### Per anatomy site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c6313c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_per_group(avg_df, \"anatomy_site\", occurences_df=occurences_df, order_by_occurrence=True, group_name=\"Anatomy Site\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db314175",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mean_and_std_over_group(avg_df, \"anatomy_site\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4546a0",
   "metadata": {},
   "source": [
    "### Per Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76232aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_per_group(avg_df, \"sex\", group_name=\"Sex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ec1ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mean_and_std_over_group(avg_df, \"sex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df77cb4d",
   "metadata": {},
   "source": [
    "### Per Age (Encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379a03c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_per_group(avg_df, \"age_encoded\", occurences_df=occurences_df, group_name=\"Age Group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d23e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mean_and_std_over_group(avg_df, \"age_encoded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f9682c",
   "metadata": {},
   "source": [
    "## Multiple Experiment Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b3536a",
   "metadata": {},
   "source": [
    "### Bar Charts for each Metric on Different variables (Anatomy Site, Age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34122d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_and_titles = [\n",
    "    # (\"../../evaluation/baseline/only_imaging_baseline_resnet34.csv\", \"ResNet34 Baseline Imaging\"),\n",
    "    (\"../../evaluation/baseline/imaging_and_clinical_baseline_resnet34.csv\", \"ResNet34 Baseline Imaging+Clinical\"),\n",
    "    # (\"../../evaluation/baseline/only_imaging_baseline_nest_small.csv\", \"Nest Small Baseline Imaging\"),\n",
    "    # (\"../../evaluation/baseline/imaging_and_clinical_baseline_nest_small.csv\", \"Nest Small Baseline Imaging+Clinical\"),\n",
    "    # (\"../../evaluation/baseline_pretrained/only_imaging_pretrained_baseline_resnet50.csv\", \"Torchxrayvision ResNet50 Imaging\"),\n",
    "    (\"../../evaluation/baseline_pretrained/imaging_and_clinical_pretrained_baseline_resnet50.csv\", \"Torchxrayvision ResNet50 Imaging+Clinical\"),\n",
    "    # (\"../../evaluation/vlp/linear_probe_only_imaging_resnet34.csv\", \"ResNet34 VLP Linear Probe Imaging\"),\n",
    "    # (\"../../evaluation/finetune/only_imaging_finetune_resnet34.csv\", \"ResNet34 Finetune Imaging\"),\n",
    "    (\"../../evaluation/finetune/imaging_and_clinical_finetune_resnet34.csv\", \"VLP Finetune ResNet34 Imaging+Clinical\")\n",
    "]\n",
    "imaging_files = [f[0] for f in files_and_titles]\n",
    "imaging_titles = [f[1] for f in files_and_titles]\n",
    "imaging_experiment_dfs = [pd.read_csv(f) for f in imaging_files]\n",
    "imaging_experiment_dfs = [clean(df) for df in imaging_experiment_dfs]\n",
    "imaging_dfs = [average_over_folds(df) for df in imaging_experiment_dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2688601",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in imaging_dfs:\n",
    "    print(df[df['group'] == 'arm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195a9f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_per_group(imaging_dfs, \"entity\", occurences_df=occurences_df, group_name=\"Entity\", exclude_most_common=True, order_by_occurrence=True, df_titles=imaging_titles, show_values=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ccde33",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = plot_metrics_per_group(imaging_dfs, \"anatomy_site\", occurences_df=occurences_df, group_name=\"Anatomy Site\", exclude_most_common=False, order_by_occurrence=True, df_titles=imaging_titles, show_values=False)\n",
    "plots['roc_auc'].savefig(\"../../visualizations/results/pretrained_baseline_vs_finetune_auroc_on_anatomy_site.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b42c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_per_group(imaging_dfs, \"age_encoded\", occurences_df=occurences_df, group_name=\"Age Encoded\", exclude_most_common=False, order_by_occurrence=False, df_titles=imaging_titles, show_values=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83782010",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_per_group(imaging_dfs, \"age_group\", occurences_df=occurences_df, group_name=\"Age Grouped\", exclude_most_common=False, order_by_occurrence=False, df_titles=imaging_titles, show_values=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759fd037",
   "metadata": {},
   "source": [
    "### Performance Latex Table Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34c98fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latex_table(avg_dfs, titles):\n",
    "    \"\"\"\n",
    "    Generate LaTeX table with overall performance metrics.\n",
    "    \"\"\"\n",
    "    # Extract overall metrics for all experiments\n",
    "    table_data = []\n",
    "    metrics_of_interest = ['accuracy', 'precision', 'recall', 'roc_auc']  # Removed f1_score\n",
    "    \n",
    "    for i, avg_df in enumerate(avg_dfs):\n",
    "        overall_metrics = avg_df[avg_df[\"level\"] == \"overall\"]\n",
    "        row_data = {'experiment': titles[i]}\n",
    "        \n",
    "        for metric in metrics_of_interest:\n",
    "            metric_row = overall_metrics[overall_metrics[\"metric\"] == metric]\n",
    "            if not metric_row.empty:\n",
    "                mean_val = metric_row[(\"value\", \"mean\")].iloc[0]\n",
    "                std_val = metric_row[(\"value\", \"std\")].iloc[0]\n",
    "                row_data[metric] = (mean_val, std_val)\n",
    "            else:\n",
    "                row_data[metric] = (0.0, 0.0)\n",
    "        \n",
    "        table_data.append(row_data)\n",
    "    \n",
    "    # Group experiments by imaging type\n",
    "    imaging_only = []\n",
    "    imaging_clinical = []\n",
    "    \n",
    "    for row in table_data:\n",
    "        if \"Only Imaging\" in row['experiment']:\n",
    "            imaging_only.append(row)\n",
    "        elif \"Imaging+Clinical\" in row['experiment'] or \"Imaging & Clinical\" in row['experiment']:\n",
    "            imaging_clinical.append(row)\n",
    "    \n",
    "    # Find best performance for each metric within each group\n",
    "    def find_best_in_group(group_data):\n",
    "        best_values = {}\n",
    "        for metric in metrics_of_interest:\n",
    "            if group_data:\n",
    "                best_mean = max(row[metric][0] for row in group_data)\n",
    "                best_values[metric] = best_mean\n",
    "        return best_values\n",
    "    \n",
    "    imaging_only_best = find_best_in_group(imaging_only)\n",
    "    imaging_clinical_best = find_best_in_group(imaging_clinical)\n",
    "    \n",
    "    # Generate LaTeX table\n",
    "    latex_code = \"\"\"\\\\begin{table*}[htbp]\n",
    "\\\\centering\n",
    "\\\\caption{Test performance across experiments.}\n",
    "\\\\label{tab:test_results}\n",
    "\\\\renewcommand{\\\\arraystretch}{1.2}\n",
    "\\\\setlength{\\\\tabcolsep}{6pt}\n",
    "\\\\begin{tabular}{llcccc}\n",
    "\\\\toprule\n",
    "\\\\textbf{Experiment} & \\\\textbf{Model} & \\\\textbf{Acc} & \\\\textbf{Prec} & \\\\textbf{Rec} & \\\\textbf{AUROC} \\\\\\\\\n",
    "\\\\midrule\"\"\"\n",
    "    \n",
    "    # Add imaging only section\n",
    "    if imaging_only:\n",
    "        latex_code += \"\\n\\\\multicolumn{6}{l}{\\\\textbf{Imaging}} \\\\\\\\\"\n",
    "        for row in imaging_only:\n",
    "            # Extract model name from experiment name\n",
    "            if \"ResNet34\" in row['experiment']:\n",
    "                model = \"ResNet34\"\n",
    "            elif \"ResNet50\" in row['experiment']:\n",
    "                model = \"ResNet50\"\n",
    "            elif \"Nest Small\" in row['experiment']:\n",
    "                model = \"NesT-S\"\n",
    "            else:\n",
    "                model = \"Unknown\"\n",
    "            \n",
    "            # Extract experiment type\n",
    "            if \"Baseline\" in row['experiment'] and \"Pretrained\" not in row['experiment'] and \"VLP\" not in row['experiment'] and \"Finetune\" not in row['experiment']:\n",
    "                exp_name = \"Baseline\"\n",
    "            elif \"Pretrained Baseline\" in row['experiment']:\n",
    "                exp_name = \"Pretrained Baseline\"\n",
    "            elif \"VLP Linear Probe\" in row['experiment'] or \"Linear Probe\" in row['experiment']:\n",
    "                exp_name = \"\\\\ac{VLP} Linear Probe (ours)\"\n",
    "            elif \"Finetune\" in row['experiment'] and \"VLP\" in row['experiment']:\n",
    "                exp_name = \"\\\\ac{VLP} Finetune (ours)\"\n",
    "            elif \"Finetune\" in row['experiment']:\n",
    "                exp_name = \"\\\\ac{VLP} Finetune (ours)\"\n",
    "            else:\n",
    "                exp_name = \"Unknown\"\n",
    "            \n",
    "            latex_code += f\"\\n{exp_name} & {model}\"\n",
    "            \n",
    "            for metric in metrics_of_interest:\n",
    "                mean_val, std_val = row[metric]\n",
    "                formatted_val = f\"{mean_val:.3f}$\\\\pm${std_val:.3f}\"\n",
    "                \n",
    "                # Make bold if best performance within imaging only group\n",
    "                if abs(mean_val - imaging_only_best[metric]) < 1e-6:\n",
    "                    formatted_val = f\"\\\\textbf{{{formatted_val}}}\"\n",
    "                \n",
    "                latex_code += f\" & {formatted_val}\"\n",
    "            \n",
    "            latex_code += \" \\\\\\\\\"\n",
    "    \n",
    "    # Add separator\n",
    "    if imaging_only and imaging_clinical:\n",
    "        latex_code += \"\\n\\\\midrule\"\n",
    "    \n",
    "    # Add imaging + clinical section\n",
    "    if imaging_clinical:\n",
    "        latex_code += \"\\n\\\\multicolumn{6}{l}{\\\\textbf{Imaging + Clinical}} \\\\\\\\\"\n",
    "        for row in imaging_clinical:\n",
    "            # Extract model name from experiment name\n",
    "            if \"ResNet34\" in row['experiment']:\n",
    "                model = \"ResNet34\"\n",
    "            elif \"ResNet50\" in row['experiment']:\n",
    "                model = \"ResNet50\"\n",
    "            elif \"Nest Small\" in row['experiment']:\n",
    "                model = \"NesT-S\"\n",
    "            else:\n",
    "                model = \"Unknown\"\n",
    "            \n",
    "            # Extract experiment type\n",
    "            if \"Baseline\" in row['experiment'] and \"Pretrained\" not in row['experiment'] and \"VLP\" not in row['experiment'] and \"Finetune\" not in row['experiment']:\n",
    "                exp_name = \"Baseline\"\n",
    "            elif \"Pretrained Baseline\" in row['experiment']:\n",
    "                exp_name = \"Pretrained Baseline\"\n",
    "            elif \"VLP Linear Probe\" in row['experiment'] or \"Linear Probe\" in row['experiment']:\n",
    "                exp_name = \"\\\\ac{VLP} Linear Probe (ours)\"\n",
    "            elif \"Finetune\" in row['experiment'] and \"VLP\" in row['experiment']:\n",
    "                exp_name = \"\\\\ac{VLP} Finetune (ours)\"\n",
    "            elif \"Finetune\" in row['experiment']:\n",
    "                exp_name = \"\\\\ac{VLP} Finetune (ours)\"\n",
    "            else:\n",
    "                exp_name = \"Unknown\"\n",
    "            \n",
    "            latex_code += f\"\\n{exp_name} & {model}\"\n",
    "            \n",
    "            for metric in metrics_of_interest:\n",
    "                mean_val, std_val = row[metric]\n",
    "                formatted_val = f\"{mean_val:.3f}$\\\\pm${std_val:.3f}\"\n",
    "                \n",
    "                # Make bold if best performance within imaging + clinical group\n",
    "                if abs(mean_val - imaging_clinical_best[metric]) < 1e-6:\n",
    "                    formatted_val = f\"\\\\textbf{{{formatted_val}}}\"\n",
    "                \n",
    "                latex_code += f\" & {formatted_val}\"\n",
    "            \n",
    "            latex_code += \" \\\\\\\\\"\n",
    "    \n",
    "    latex_code += \"\"\"\n",
    "\\\\bottomrule\n",
    "\\\\end{tabular}\n",
    "\\\\end{table*}\"\"\"\n",
    "    \n",
    "    return latex_code\n",
    "\n",
    "# Generate and display the LaTeX table\n",
    "latex_table = generate_latex_table(imaging_dfs, imaging_titles)\n",
    "print(\"LaTeX Table Code:\")\n",
    "print(\"=\"*50)\n",
    "print(latex_table)\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Also create a summary DataFrame for easier viewing\n",
    "summary_data = []\n",
    "metrics_of_interest = ['accuracy', 'precision', 'recall', 'roc_auc']  # Removed f1_score\n",
    "\n",
    "for i, avg_df in enumerate(imaging_dfs):\n",
    "    overall_metrics = avg_df[avg_df[\"level\"] == \"overall\"]\n",
    "    row_data = {'Experiment': imaging_titles[i]}\n",
    "    \n",
    "    for metric in metrics_of_interest:\n",
    "        metric_row = overall_metrics[overall_metrics[\"metric\"] == metric]\n",
    "        if not metric_row.empty:\n",
    "            mean_val = metric_row[(\"value\", \"mean\")].iloc[0]\n",
    "            std_val = metric_row[(\"value\", \"std\")].iloc[0]\n",
    "            row_data[metric.replace('_', ' ').title()] = f\"{mean_val:.3f} ± {std_val:.3f}\"\n",
    "        else:\n",
    "            row_data[metric.replace('_', ' ').title()] = \"N/A\"\n",
    "    \n",
    "    summary_data.append(row_data)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nSummary Table (for reference):\")\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf975bb4",
   "metadata": {},
   "source": [
    "### Line Charts for each Metric on Anatomy Site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b462be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_only_imaging_resnet34 = average_over_folds(clean(pd.read_csv(\"../../evaluation/baseline/only_imaging_baseline_resnet34.csv\"))) # ResNet34 Baseline Only Imaging\n",
    "baseline_imaging_clinical_resnet34 = average_over_folds(clean(pd.read_csv(\"../../evaluation/baseline/imaging_and_clinical_baseline_resnet34.csv\"))) # ResNet34 Baseline Imaging+Clinical\n",
    "baseline_only_imaging_nest = average_over_folds(clean(pd.read_csv(\"../../evaluation/baseline/only_imaging_baseline_nest_small.csv\"))) # Nest Small Baseline Only Imaging\n",
    "baseline_imaging_clinical_nest = average_over_folds(clean(pd.read_csv(\"../../evaluation/baseline/imaging_and_clinical_baseline_nest_small.csv\"))) # Nest Small Baseline Imaging+Clinical\n",
    "pretrained_baseline_only_imaging_resnet50 = average_over_folds(clean(pd.read_csv(\"../../evaluation/baseline_pretrained/only_imaging_pretrained_baseline_resnet50.csv\"))) # ResNet50 Pretrained Baseline Only Imaging\n",
    "pretrained_baseline_imaging_clinical_resnet50 = average_over_folds(clean(pd.read_csv(\"../../evaluation/baseline_pretrained/imaging_and_clinical_pretrained_baseline_resnet50.csv\"))) # ResNet50 Pretrained Baseline Imaging+Clinical\n",
    "linear_probe_only_imaging_resnet34 = average_over_folds(clean(pd.read_csv(\"../../evaluation/vlp/linear_probe_only_imaging_resnet34.csv\"))) # ResNet34 VLP Linear Probe Only Imaging\n",
    "finetune_ony_imaging_resnet34 =  average_over_folds(clean(pd.read_csv(\"../../evaluation/finetune/only_imaging_finetune_resnet34.csv\"))) # ResNet34 Finetune Only Imaging\n",
    "finetune_imaging_clincial_resnet34 = average_over_folds(clean(pd.read_csv(\"../../evaluation/finetune/imaging_and_clinical_finetune_resnet34.csv\"))) # ResNet34 Finetune Imaging+Clinical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eabf29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_and_titles = ([\n",
    "        baseline_only_imaging_resnet34,\n",
    "        baseline_imaging_clinical_resnet34,\n",
    "        # baseline_only_imaging_nest,\n",
    "        # baseline_imaging_clinical_nest,\n",
    "        pretrained_baseline_only_imaging_resnet50,\n",
    "        pretrained_baseline_imaging_clinical_resnet50,\n",
    "        finetune_ony_imaging_resnet34,\n",
    "        finetune_imaging_clincial_resnet34,\n",
    "        # linear_probe_only_imaging_resnet34\n",
    "    ], [\n",
    "        'Baseline ResNet34 Imaging',\n",
    "        'Baseline ResNet34 Imaging+Clinical',\n",
    "        # 'Baseline Only Imaging NesT-S',\n",
    "        # 'Baseline Imaging+Clinical NesT-S',\n",
    "        'Torchxrayvision ResNet50 Imaging',\n",
    "        'Torchxrayvision ResNet50 Imaging+Clinical',\n",
    "        'VLP Finetune ResNet34 Imaging',\n",
    "        'VLP Finetune ResNet34 Imaging+Clinical',\n",
    "        # 'VLP Linear Probe Only Imaging ResNet34'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a271b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_comparison_by_var(dfs, titles, metric, var, metadata_df=None):\n",
    "    for i in range(len(dfs)):\n",
    "        dfs[i]['experiment'] = titles[i]\n",
    "\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    # only consider metric entries\n",
    "    combined_df = combined_df[combined_df['metric'] == metric]\n",
    "    # only consider the specified var\n",
    "    combined_df = combined_df[combined_df['level'] == var]\n",
    "\n",
    "    # if occurences_df is provided, sort ascending by occurence of var\n",
    "    order = None\n",
    "    percentage_data = None\n",
    "    if metadata_df is not None:\n",
    "        # drop everything in metadata_df except for the column called var\n",
    "        metadata_df = metadata_df[[var]]\n",
    "        # count occurrences of each value in var\n",
    "        group_occurrences = metadata_df.groupby(var).size().reset_index(name='count')\n",
    "        group_occurrences = group_occurrences.sort_values(by='count')\n",
    "        order = group_occurrences[var].tolist()\n",
    "        # order combined_df accordingly\n",
    "        combined_df['group'] = pd.Categorical(combined_df['group'], categories=order, ordered=True)\n",
    "        combined_df = combined_df.sort_values(by='group')\n",
    "        \n",
    "        # Prepare percentage data for secondary axis\n",
    "        total_count = group_occurrences['count'].sum()\n",
    "        percentage_data = (group_occurrences.set_index(var)['count'] / total_count)\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Primary plot (metric values)\n",
    "    plot = sns.lineplot(\n",
    "        data=combined_df,\n",
    "        x='group',\n",
    "        y=('value', 'mean'),\n",
    "        hue='experiment',\n",
    "        hue_order=titles,\n",
    "        palette=[c for i, c in enumerate(sns.color_palette(\"Paired\", len(titles) + 3)) if i not in (6, 7, 10)],\n",
    "        marker='.',\n",
    "        legend='full',\n",
    "        sort=False, # sorting is already, handled, otherwise seaborn sorts alphabetically again\n",
    "        linewidth=1,\n",
    "        ax=ax1\n",
    "    )\n",
    "\n",
    "    # plot.set_style(\"whitegrid\")\n",
    "    ax1.set_xlabel(var.replace('_', ' ').title())\n",
    "    ax1.set_ylim(0, 1.05)\n",
    "    ax1.set_ylabel(metric.replace('_', ' ').title() if metric != 'roc_auc' else 'AUROC')\n",
    "    ax1.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plot.set_ylabel(metric.replace('_', ' ').title() if metric != 'roc_auc' else 'AUROC')\n",
    "    plot.legend(loc='lower center', bbox_to_anchor=(0.5, 1.02), frameon=True)\n",
    "    \n",
    "    # Add secondary y-axis for percentage data\n",
    "    if percentage_data is not None:\n",
    "        ax2 = ax1.twinx()\n",
    "        \n",
    "        # Plot percentage line\n",
    "        groups_in_order = combined_df['group'].cat.categories if hasattr(combined_df['group'], 'cat') else combined_df['group'].unique()\n",
    "        percentages = [percentage_data.get(group, 0) for group in groups_in_order]\n",
    "        \n",
    "        ax2.plot(range(len(groups_in_order)), percentages, \n",
    "                color='gray', linestyle='--', linewidth=1, marker='o', \n",
    "                markersize=3, label='Percentage of Downstream Dataset', alpha=0.4)\n",
    "        \n",
    "        ax2.set_ylabel('Percentage', color='gray')\n",
    "        ax2.tick_params(axis='y', labelcolor='gray')\n",
    "        ax2.set_ylim(0, max(percentages) * 1.2)\n",
    "        \n",
    "        # Remove grid lines from secondary axis\n",
    "        ax2.grid(False)\n",
    "        \n",
    "        # Add legend for percentage line\n",
    "        ax2.legend(loc='lower right')\n",
    "\n",
    "    fig.set_dpi(1000)\n",
    "    plt.tight_layout()\n",
    "    plot_to_return = plt.gcf()\n",
    "    plt.show()\n",
    "    return plot_to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fec7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs, titles = dfs_and_titles\n",
    "plot = plot_metric_comparison_by_var(dfs, titles, metric='roc_auc', var='anatomy_site', metadata_df=metadata_df)\n",
    "plot.savefig(\"../../visualizations/results/auroc_over_anatomy_sites.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4a7e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs, titles = dfs_and_titles\n",
    "plot = plot_metric_comparison_by_var(dfs, titles, metric='recall', var='anatomy_site', metadata_df=metadata_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63906c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs, titles = dfs_and_titles\n",
    "plot = plot_metric_comparison_by_var(dfs, titles, metric='precision', var='anatomy_site', metadata_df=metadata_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9d6761",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs, titles = dfs_and_titles\n",
    "plot = plot_metric_comparison_by_var(dfs, titles, metric='balanced_accuracy', var='anatomy_site', metadata_df=metadata_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e28d7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs, titles = dfs_and_titles\n",
    "plot = plot_metric_comparison_by_var(dfs, titles, metric='accuracy', var='anatomy_site', metadata_df=metadata_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf772598",
   "metadata": {},
   "source": [
    "### Comparing Imaging and Imaging+Clinical Experiments on Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297bb469",
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging_files_and_titles = [\n",
    "    (\"../../evaluation/baseline/only_imaging_baseline_resnet34.csv\", \"ResNet34 Baseline Only Imaging\"),\n",
    "    (\"../../evaluation/baseline/only_imaging_baseline_nest_small.csv\", \"Nest Small Baseline Only Imaging\"),\n",
    "    (\"../../evaluation/baseline_pretrained/only_imaging_pretrained_baseline_resnet50.csv\", \"ResNet50 Pretrained Baseline Only Imaging\"),\n",
    "    (\"../../evaluation/finetune/only_imaging_finetune_resnet34.csv\", \"ResNet34 Finetune Only Imaging\"),\n",
    "]\n",
    "imaging_and_clinical_files_and_titles = [\n",
    "    (\"../../evaluation/baseline/imaging_and_clinical_baseline_resnet34.csv\", \"ResNet34 Baseline Imaging+Clinical\"),\n",
    "    (\"../../evaluation/baseline/imaging_and_clinical_baseline_nest_small.csv\", \"Nest Small Baseline Imaging+Clinical\"),\n",
    "    (\"../../evaluation/baseline_pretrained/imaging_and_clinical_pretrained_baseline_resnet50.csv\", \"ResNet50 Pretrained Baseline Imaging+Clinical\"),\n",
    "    (\"../../evaluation/finetune/imaging_and_clinical_finetune_resnet34.csv\", \"ResNet34 Finetune Imaging+Clinical\")\n",
    "]\n",
    "\n",
    "def read_clean_and_average(files_and_titles):\n",
    "    files = [f[0] for f in files_and_titles]\n",
    "    titles = [f[1] for f in files_and_titles]\n",
    "    dfs = [pd.read_csv(f) for f in files]\n",
    "    dfs = [clean(df) for df in dfs]\n",
    "    dfs = [average_over_folds(df) for df in dfs]\n",
    "    return dfs, titles\n",
    "\n",
    "imaging_dfs, imaging_titles = read_clean_and_average(imaging_files_and_titles)\n",
    "imaging_and_clinical_dfs, imaging_and_clinical_titles = read_clean_and_average(imaging_and_clinical_files_and_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cacb811",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "\n",
    "def plot_grouped_metric_comparison_by_var(group_1_dfs, group_2_dfs, group_1_title, group_2_title, metric='precision', var='age_encoded'):\n",
    "    \"\"\"\n",
    "    Plot comparison of a specified metric by age group between imaging-only and imaging+clinical experiments.\n",
    "    \n",
    "    Args:\n",
    "        group_1_dfs: List of dataframes with first group experiment results (e.g., imaging-only)\n",
    "        group_2_dfs: List of dataframes with second group experiment results (e.g., imaging+clinical)\n",
    "        metric: Metric to analyze (default: 'precision')\n",
    "        age_level: Age grouping level to use - 'age_encoded' or 'age_group' (default: 'age_encoded')\n",
    "    \"\"\"\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Combine first group experiments (keep experiment ID for std calculation)\n",
    "    group_1_combined = []\n",
    "    for i in range(len(group_1_dfs)):\n",
    "        df_copy = group_1_dfs[i].copy()\n",
    "        df_copy['experiment'] = i\n",
    "        group_1_combined.append(df_copy)\n",
    "    \n",
    "    group_1_experiment_dfs = pd.concat(group_1_combined, ignore_index=True)\n",
    "    group_2_combined = []\n",
    "    for i in range(len(group_2_dfs)):\n",
    "        df_copy = group_2_dfs[i].copy()\n",
    "        df_copy['experiment'] = i + len(group_1_dfs)\n",
    "        group_2_combined.append(df_copy)\n",
    "    \n",
    "    group_2_experiment_dfs = pd.concat(group_2_combined, ignore_index=True)\n",
    "\n",
    "    # Extract age data for specified metric and age level\n",
    "    group_1_age_metric = group_1_experiment_dfs[\n",
    "        (group_1_experiment_dfs['level'] == var) & \n",
    "        (group_1_experiment_dfs['metric'] == metric)\n",
    "    ].copy()\n",
    "\n",
    "    group_2_age_metric = group_2_experiment_dfs[\n",
    "        (group_2_experiment_dfs['level'] == var) & \n",
    "        (group_2_experiment_dfs['metric'] == metric)\n",
    "    ].copy()\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    group_1_age_metric['Comparison Group'] = group_1_title\n",
    "    group_2_age_metric['Comparison Group'] = group_2_title\n",
    "\n",
    "    # Combine both dataframes\n",
    "    combined_data = pd.concat([group_1_age_metric, group_2_age_metric], ignore_index=True)\n",
    "\n",
    "    # Rename columns for easier access\n",
    "    age_level_titel = var.replace('_', ' ').title() if var != 'age_encoded' else 'Age'\n",
    "    combined_data = combined_data.rename(columns={\n",
    "        'level': 'level',\n",
    "        'group': age_level_titel,\n",
    "        'metric': 'metric',\n",
    "        'value': metric\n",
    "    })\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plot = sns.lineplot(\n",
    "        data=combined_data,\n",
    "        x=age_level_titel,\n",
    "        y=(metric, 'mean'),\n",
    "        hue='Comparison Group',\n",
    "        estimator=np.mean,\n",
    "        marker='o',\n",
    "        errorbar='sd',\n",
    "        err_style='band',\n",
    "        legend='full'\n",
    "    )\n",
    "\n",
    "    if var == 'age_encoded':\n",
    "        plot.set_xticks([0, 1, 2, 3, 4, 5, 6], ['0-9', '10-19', '20-29', '30-39', '40-49', '50-59', '60+'])\n",
    "\n",
    "    plot.set_ylim(0, 1)\n",
    "    plot.set_ylabel(metric.replace(\"_\", \" \").title() if metric != 'roc_auc' else 'AUROC')\n",
    "    plot.legend(loc='lower left')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed6750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plot_grouped_metric_comparison_by_var(\n",
    "    imaging_dfs, \n",
    "    imaging_and_clinical_dfs,\n",
    "    group_1_title='Imaging',\n",
    "    group_2_title='Imaging + Clinical',\n",
    "    metric='precision',\n",
    "    var='age_encoded'\n",
    ")\n",
    "# plot = plot_grouped_metric_comparison_by_var(\n",
    "#     imaging_dfs, \n",
    "#     imaging_and_clinical_dfs,\n",
    "#     group_1_title='Imaging',\n",
    "#     group_2_title='Imaging + Clinical',\n",
    "#     metric='precision',\n",
    "#     var='age_group'\n",
    "# )\n",
    "plot.savefig(\"../../visualizations/results/imaging_vs_imaging_and_clinical_across_age_encoded_precision.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a40d3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plot_grouped_metric_comparison_by_var(\n",
    "    imaging_dfs, \n",
    "    imaging_and_clinical_dfs,\n",
    "    group_1_title='Imaging',\n",
    "    group_2_title='Imaging + Clinical',\n",
    "    metric='recall',\n",
    "    var='age_encoded'\n",
    ")\n",
    "plot.savefig(\"../../visualizations/results/imaging_vs_imaging_and_clinical_across_age_encoded_recall.svg\")\n",
    "# plot_grouped_metric_comparison_by_var(\n",
    "#     imaging_dfs, \n",
    "#     imaging_and_clinical_dfs,\n",
    "#     group_1_title='Imaging',\n",
    "#     group_2_title='Imaging + Clinical',\n",
    "#     metric='recall',\n",
    "#     var='age_group'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32a7d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plot_grouped_metric_comparison_by_var(\n",
    "    imaging_dfs, \n",
    "    imaging_and_clinical_dfs,\n",
    "    group_1_title='Imaging',\n",
    "    group_2_title='Imaging + Clinical',\n",
    "    metric='f1_score',\n",
    "    var='age_encoded'\n",
    ")\n",
    "plot.savefig(\"../../visualizations/results/imaging_vs_imaging_and_clinical_across_age_encoded_f1_score.svg\")\n",
    "# plot_grouped_metric_comparison_by_var(\n",
    "#     imaging_dfs, \n",
    "#     imaging_and_clinical_dfs,\n",
    "#     group_1_title='Imaging',\n",
    "#     group_2_title='Imaging + Clinical',\n",
    "#     metric='f1_score',\n",
    "#     var='age_group'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41c236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plot_grouped_metric_comparison_by_var(\n",
    "    imaging_dfs, \n",
    "    imaging_and_clinical_dfs,\n",
    "    group_1_title='Imaging',\n",
    "    group_2_title='Imaging + Clinical',\n",
    "    metric='roc_auc',\n",
    "    var='age_encoded'\n",
    ")\n",
    "plot.savefig(\"../../visualizations/results/imaging_vs_imaging_and_clinical_across_age_encoded_auroc.svg\")\n",
    "# plot_grouped_metric_comparison_by_var(\n",
    "#     imaging_dfs, \n",
    "#     imaging_and_clinical_dfs,\n",
    "#     group_1_title='Imaging',\n",
    "#     group_2_title='Imaging + Clinical',\n",
    "#     metric='roc_auc',\n",
    "#     var='age_group'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be762f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grouped_metric_comparison_by_var(\n",
    "    imaging_dfs, \n",
    "    imaging_and_clinical_dfs,\n",
    "    group_1_title='Imaging',\n",
    "    group_2_title='Imaging + Clinical',\n",
    "    metric='precision',\n",
    "    var='anatomy_site'\n",
    ")\n",
    "plot_grouped_metric_comparison_by_var(\n",
    "    imaging_dfs, \n",
    "    imaging_and_clinical_dfs,\n",
    "    group_1_title='Imaging',\n",
    "    group_2_title='Imaging + Clinical',\n",
    "    metric='recall',\n",
    "    var='anatomy_site'\n",
    ")\n",
    "plot_grouped_metric_comparison_by_var(\n",
    "    imaging_dfs, \n",
    "    imaging_and_clinical_dfs,\n",
    "    group_1_title='Imaging',\n",
    "    group_2_title='Imaging + Clinical',\n",
    "    metric='roc_auc',\n",
    "    var='anatomy_site'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532e4b81",
   "metadata": {},
   "source": [
    "#### Have a look at precison over age compared between Scratch and Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3e3611",
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch_files_and_titles = [\n",
    "    (\"../../evaluation/baseline/only_imaging_baseline_resnet34.csv\", \"ResNet34 Baseline Only Imaging\"),\n",
    "    (\"../../evaluation/baseline/imaging_and_clinical_baseline_resnet34.csv\", \"ResNet34 Baseline Imaging+Clinical\"),\n",
    "    # (\"../../evaluation/baseline/only_imaging_baseline_nest_small.csv\", \"Nest Small Baseline Only Imaging\"),\n",
    "    # (\"../../evaluation/baseline_pretrained/only_imaging_pretrained_baseline_resnet50.csv\", \"ResNet50 Pretrained Baseline Only Imaging\"),\n",
    "]\n",
    "finetune_files_and_titles = [\n",
    "    # (\"../../evaluation/baseline/imaging_and_clinical_baseline_nest_small.csv\", \"Nest Small Baseline Imaging+Clinical\"),\n",
    "    # (\"../../evaluation/baseline_pretrained/imaging_and_clinical_pretrained_baseline_resnet50.csv\", \"ResNet50 Pretrained Baseline Imaging+Clinical\"),\n",
    "    (\"../../evaluation/finetune/only_imaging_finetune_resnet34.csv\", \"ResNet34 Finetune Only Imaging\"),\n",
    "    (\"../../evaluation/finetune/imaging_and_clinical_finetune_resnet34.csv\", \"ResNet34 Finetune Imaging+Clinical\")\n",
    "]\n",
    "\n",
    "scratch_dfs, scratch_titles = read_clean_and_average(scratch_files_and_titles)\n",
    "finetune_dfs, finetune_titles = read_clean_and_average(finetune_files_and_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f90593",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grouped_metric_comparison_by_var(\n",
    "    scratch_dfs, \n",
    "    finetune_dfs,\n",
    "    group_1_title='Scratch',\n",
    "    group_2_title='Finetuning',\n",
    "    metric='precision',\n",
    "    var='age_encoded'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60b2398",
   "metadata": {},
   "source": [
    "### Significance test, whether pretrained baseline and finetune differ significantly in their predictions\n",
    "\n",
    ">Note: only considering Imaging+Clinical here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ac30c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take fold number 1 from pretrained baseline, since it has best auroc on test set compared to the other model folds\n",
    "probs_df_pretrained_baseine = pd.read_csv('../../predictions/baseline_pretrained/imaging_and_clinical_pretrained_baseline_resnet50/predictions_fold_1.csv')\n",
    "probs_df_pretrained_baseine = pd.read_csv('../../predictions/baseline/imaging_and_clinical_baseline_resnet34/predictions_fold_0.csv')\n",
    "probs_pretrained_baseline = probs_df_pretrained_baseine['prob']\n",
    "\n",
    "\n",
    "# take fold number 2 from finetuned (ourse), same reason\n",
    "probs_df_finetuned = pd.read_csv('../../predictions/finetune/imaging_and_clinical_finetune_resnet34/predictions_fold_2.csv')\n",
    "probs_finetuned = probs_df_finetuned['prob']\n",
    "\n",
    "# we assume that the order of samples is the same in both prediction files -> s.t. we can do a paired test\n",
    "# having a sanity check by comparing wether everything except the probabilities is the same\n",
    "cols_to_check = ['dataset', 'entity', 'anatomy_site', 'sex', 'age', 'age_encoded', 'tumor']\n",
    "for col in cols_to_check:\n",
    "    if not all(probs_df_pretrained_baseine[col] == probs_df_finetuned[col]):\n",
    "        raise ValueError(f\"Column {col} does not match between the two prediction files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed28d24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import normaltest\n",
    "\n",
    "stat, p_value = normaltest(probs_pretrained_baseline - probs_finetuned)\n",
    "print(f\"\\nNormality test for difference of probabilities:\")\n",
    "print(f\"Statistic: {stat}, p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58617599",
   "metadata": {},
   "source": [
    "p-value < 0.05 so we reject the null hypothesis that the data is normally distributed -> t-test not applicable.\n",
    "Instead, we use the wilcoxon signed-rank test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de627085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "\n",
    "stat, p_value = wilcoxon(probs_pretrained_baseline, probs_finetuned, alternative='two-sided')\n",
    "print(f\"Wilcoxon Signed Rank Test for difference in pretrained baseline and finetune\")\n",
    "print(f\"Statistic: {stat}, p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e845379a",
   "metadata": {},
   "source": [
    "p_value < 0.05 so we reject the null hypothesis that both data lists are from the same distribution -> They differ significantly in their predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f3bc35",
   "metadata": {},
   "source": [
    "## Investigating Metadata Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c64f3a",
   "metadata": {},
   "source": [
    "I'm interested in whether the fusion approach using the metadata: age, sex_encoded, and anatomy site, can leverage combinations of metadata, where there is only one label at all (e.g. for male, age_encoded 4, knee -> there might only be non-tumor samples). For that I'm having a look whether such groups exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5194c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_age(age: int):\n",
    "    if age < 0:\n",
    "        raise ValueError(f\"Age must be a positive integer, got {age}\")\n",
    "\n",
    "    # as described in Michaels thesis the age is binned into 10 year interval with all 60 and above assigned to the same bin 7\n",
    "    if age < 10:\n",
    "        bin = 1\n",
    "    elif age < 20:\n",
    "        bin = 2\n",
    "    elif age < 30:\n",
    "        bin = 3\n",
    "    elif age < 40:\n",
    "        bin = 4\n",
    "    elif age < 50:\n",
    "        bin = 5\n",
    "    elif age < 60:\n",
    "        bin = 6\n",
    "    else:\n",
    "        bin = 7\n",
    "\n",
    "    return bin\n",
    "\n",
    "metadata_df['age_encoded'] = metadata_df['age'].apply(encode_age)\n",
    "metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9614fa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop age, datasetset, and entity since the models actually never see this\n",
    "skinny_metadata_df = metadata_df.drop(['dataset', 'entity', 'age', 'set'], axis=1)\n",
    "skinny_metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86c1281",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_groups = skinny_metadata_df.groupby(['anatomy_site', 'sex', 'age_encoded']).agg({'tumor': ['sum', 'count']})\n",
    "metadata_groups.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c067b4",
   "metadata": {},
   "source": [
    "In two scenarios as there only one possible tumor label per metadata combination: If the sum is 0 -> all have tumor label 0 or if sum == count -> all have tumor label 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6992b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_one_label_present = (metadata_groups['tumor']['sum'] == 0) | (metadata_groups['tumor']['sum'] == metadata_groups['tumor']['count'])\n",
    "print(f\"{sum(only_one_label_present)}/{len(only_one_label_present)} metadata combinations have only one tumor label present\")\n",
    "\n",
    "only_one_label_present_and_more_than_2 = (metadata_groups['tumor']['sum'] == 0) | (metadata_groups['tumor']['sum'] == metadata_groups['tumor']['count']) & (metadata_groups['tumor']['count'] > 2)\n",
    "print(f\"{sum(only_one_label_present_and_more_than_2)}/{len(only_one_label_present_and_more_than_2)} metadata combinations with 3 or more representatives, have only one tumor label present\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fc782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the amount of samples for which its combination has only one label present\n",
    "samples_with_only_one_label_present = metadata_groups[only_one_label_present]\n",
    "samples_with_only_one_label_present = samples_with_only_one_label_present['tumor']['count'].sum()\n",
    "\n",
    "print(f\"{samples_with_only_one_label_present}/{metadata_groups['tumor']['count'].sum()} samples have a combination of metadata features with only one tumor label present\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834cf66a",
   "metadata": {},
   "source": [
    "## Calculating Specificity and Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7ad063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# load predictions\n",
    "# pretrained baseline\n",
    "files = ['../../predictions/baseline_pretrained/imaging_and_clinical_pretrained_baseline_resnet50/predictions_fold_0.csv',\n",
    "         '../../predictions/baseline_pretrained/imaging_and_clinical_pretrained_baseline_resnet50/predictions_fold_1.csv',\n",
    "         '../../predictions/baseline_pretrained/imaging_and_clinical_pretrained_baseline_resnet50/predictions_fold_2.csv',\n",
    "         '../../predictions/baseline_pretrained/imaging_and_clinical_pretrained_baseline_resnet50/predictions_fold_3.csv'\n",
    "    ]\n",
    "dfs = [pd.read_csv(f) for f in files]\n",
    "sensitivities = []\n",
    "specificities = []\n",
    "precisions = []\n",
    "for df in dfs:\n",
    "    df['pred'] = df['prob'] >= 0.5\n",
    "    tn, fp, fn, tp = confusion_matrix(df['tumor'], df['pred']).ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    precision = tp / (tp + fp)\n",
    "    sensitivities.append(sensitivity)\n",
    "    specificities.append(specificity)\n",
    "    precisions.append(precision)\n",
    "\n",
    "# average over folds +- std\n",
    "print(f\"Sensitivity: {np.mean(sensitivities):.4f} ± {np.std(sensitivities):.4f}\")\n",
    "print(f\"Specificity: {np.mean(specificities):.4f} ± {np.std(specificities):.4f}\")\n",
    "print(f\"Precision: {np.mean(precisions):.4f} ± {np.std(precisions):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
