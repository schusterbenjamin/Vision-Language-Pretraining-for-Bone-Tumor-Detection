program: src/train.py
method: bayes
metric:
  goal: minimize
  name: val/combined/loss
parameters:
  scheduler:
    values: ["no_scheduler", "cosine", "cosine_with_warmup"]
  optimizer:
    values: ["adamw", "adam"]
  optimizer.lr:
    min: !!float 1e-7
    max: !!float 1e-3
  data.batch_size:
    values: [8, 16, 32, 64, 128, 256]
command:
  - python
  - ${program}
# Defining fixed parameters directly in the command (I dont want to have to specify a new hydra config file for each sweep)
  - seed=42
  - trainer.max_epochs=300 # 300 taken from Michaels master's thesis
  - callbacks=baseline_final
  - model=fusion
  - model.model=resnet34
  - ++model.coral_lambda=1000
  - ++model.pretrained_vlp_module="outputs/2025-09-07/21-44-01/vision-language-bone-tumor-pretraining-reloaded/py9knd36/checkpoints/vlp-linear_probe_acc-epoch:60-downstream_val_linear_probe_acc:0.712550699710846.ckpt"
  - ${args_no_hyphens}
